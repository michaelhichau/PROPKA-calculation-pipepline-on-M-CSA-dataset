{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import sys, traceback\n",
    "import subprocess\n",
    "import json\n",
    "from decimal import Decimal as D\n",
    "import requests\n",
    "\n",
    "\n",
    "# import re\n",
    "# import Bio.PDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<span style=\"Times New Roman; font-size:2em;\">**Loading data**</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "response = requests.get('https://www.ebi.ac.uk/thornton-srv/m-csa/api/residues/?format=json')\n",
    "pd.DataFrame(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "curated_csv_data = []\n",
    "with open('curated_data.csv', 'r') as csv_file:\n",
    "    for line in csv_file:\n",
    "        row = line.replace('\\n','').split(',')\n",
    "        curated_csv_data.append(row[:12])\n",
    "\n",
    "curated_df = pd.DataFrame(curated_csv_data[1:], columns = ['M-CSA ID',\n",
    " 'Uniprot IDs',\n",
    " 'PDB ID',\n",
    " 'EC',\n",
    " 'residue/reactant/product/cofactor',\n",
    " 'RESIDUE TYPE',\n",
    " 'CHAIN ID',\n",
    " 'RESIDUE NUMBER',\n",
    " 'function location/name',\n",
    " 'ROLE',\n",
    " 'ROLE_TYPE',\n",
    " 'PARENT ROLE'])\n",
    "\n",
    "# curated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Find the counts of a column\n",
    "pd.DataFrame(curated_df['RESIDUE TYPE'].value_counts()).to_csv( 'RESIDUE TYPE count.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "residues_roles_df = pd.read_csv('literature_pdb_residues_roles.csv')\n",
    "# residues_roles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(residues_roles_df.info())\n",
    "# print(residues_roles_df['RESIDUE TYPE'].unique())\n",
    "# print(residues_roles_df['ROLE_TYPE'].unique())\n",
    "# print(residues_roles_df['ROLE'].unique())\n",
    "# print(residues_roles_df['PARENT ROLE'].unique())\n",
    "\n",
    "ls = len(residues_roles_df['PDB ID'].unique())\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "pdb_directory = 'pdb/\\'batch download structures\\'/'\n",
    "result_directory = 'pdb/batch_result/'\n",
    "csv_directory = 'pdb/batch_csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949\n"
     ]
    }
   ],
   "source": [
    "# Unzipping .gz files\n",
    "# os.system('gunzip -d *.ent.gz')\n",
    "\n",
    "# Renaming .ent files as pdb\n",
    "# os.system('rename.ul pdb ''  *.ent')\n",
    "# os.system('rename.ul .ent .pdb *.ent')\n",
    "\n",
    "# checking the files in the given directory\n",
    "stream = os.popen('ls '+ pdb_directory)\n",
    "output = stream.read()\n",
    "stream.close()\n",
    "list_of_files = output.splitlines()\n",
    "list_of_files = [file_name.replace('.pdb','') for file_name in list_of_files]\n",
    "print(len(list_of_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# checking the difference in the list of enzymes\n",
    "print(set(ls) - set(list_of_files))\n",
    "print(set(list_of_files) - set(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# A dictionary to record the problems encountered\n",
    "error_log = dict()\n",
    "f = open(\"error_log.txt\", \"r\")\n",
    "error_log = json.load(f)\n",
    "f.close()\n",
    "\n",
    "# An array to record the tasks completed\n",
    "completed_tasks = []\n",
    "f = open(\"completed_tasks.txt\", \"r\")\n",
    "for line in f:\n",
    "    completed_tasks.append(line.replace('\\n',''))\n",
    "f.close()\n",
    "\n",
    "# An Array to record to tasks remaining\n",
    "remaining_tasks = list(set(list_of_files) - set(completed_tasks))\n",
    "remaining_tasks.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<span style=\"Times New Roman; font-size:2em;\">**pKa Calculation**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def pka_calculate(pdb_id):\n",
    "    # Calculation with propka3\n",
    "    output = subprocess.run([\"propka3\", pdb_directory.replace('\\'','') + pdb_id + '.pdb'], capture_output=True)\n",
    "    print(output.stdout.decode(\"utf-8\"))\n",
    "    f = open(\"log_sheet.txt\", \"a\")\n",
    "    f.write(output.stdout.decode(\"utf-8\") + '\\n')\n",
    "    f.close()\n",
    "\n",
    "    # Leaving a record in case of error\n",
    "    if output.stderr.decode(\"utf-8\") != '':\n",
    "        error_log[pdb_id] = output.stderr.decode(\"utf-8\")\n",
    "        print(json.dumps(error_log, indent=2))\n",
    "        f = open(\"error_log.txt\", \"w\")\n",
    "        f.write(json.dumps(error_log, indent=2))\n",
    "        f.close()\n",
    "    else:\n",
    "        # Move the file if everything worked\n",
    "        os.system('mv ' + pdb_id + '.pka ' + result_directory[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# def pka_to_pandas(pdb_id):\n",
    "\n",
    "#     # Check whether the directory exists\n",
    "#     file_name = result_directory + pdb_id + '.pka'\n",
    "\n",
    "#     path_exists = Path(file_name).is_file()\n",
    "\n",
    "#     if(not path_exists) :\n",
    "#         print(\"File path not found:\" , file_name)\n",
    "#         f = open(\"log_sheet.txt\", \"a\")\n",
    "#         f.write('File path not found: ' + file_name + '\\n')\n",
    "#         f.close()\n",
    "#         error_log[pdb_id] = \"File path not found:\" + file_name\n",
    "\n",
    "#     else:\n",
    "#         # Reading the file if it exists\n",
    "#         # Combine the file content as one string\n",
    "#         file_text = ''\n",
    "#         for line in open(file_name, 'r'):\n",
    "#             file_text += line\n",
    "\n",
    "#         # Splitting the result file by section\n",
    "#         section = file_text.split('--------------------------------------------------------------------------------------------------------')\n",
    "#         result_splitlines = section[1].splitlines()\n",
    "\n",
    "#         # An array to store the result table\n",
    "#         results = []\n",
    "#         for string in result_splitlines[3:]:\n",
    "# #             splitted_line = string.split()\n",
    "# #             if len(splitted_line) < 6:\n",
    "# #                 splitted_line.append('')\n",
    "# #             results.append(splitted_line)\n",
    "#         #     The following line is reserved for splitting the table by fixed length\n",
    "#             results.append([string[0:6].lstrip(),string[6:10].lstrip(),string[10:12].lstrip(),string[12:21].lstrip(),string[21:32].lstrip(),string[32:55].lstrip()])\n",
    "\n",
    "#         # Loading the array to pd dataframe\n",
    "#         single_result_df = pd.DataFrame(results, columns = ['RESIDUE TYPE', 'RESIDUE NUMBER', 'CHAIN ID', 'pKa', 'model-pKa', 'ligand atom-type'])\n",
    "\n",
    "#         # Adding a column indicating the PDB ID\n",
    "#         single_result_df.insert(0, 'PDB ID', pdb_id)\n",
    "\n",
    "#         # Write csv file\n",
    "#         single_result_df.to_csv( csv_directory + pdb_id + '.csv', index=False)\n",
    "\n",
    "#         # return the results\n",
    "#         return single_result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Dataframe for storing the results\n",
    "# results_df = pd.DataFrame()\n",
    "\n",
    "# Running calculation for all files /Takes a LOOONG TIME (1 hour+)\n",
    "for pdb_id in remaining_tasks:\n",
    "    # Logging events\n",
    "    print('Running:', pdb_id)\n",
    "    f = open(\"log_sheet.txt\", \"a\")\n",
    "    f.write('Running: ' + pdb_id + '\\n')\n",
    "    f.close()\n",
    "\n",
    "    # Calculate and formatting the output\n",
    "    pka_calculate(pdb_id)\n",
    "#     results_df = pd.concat([results_df, pka_to_pandas(pdb_id)])\n",
    "\n",
    "#     # Logging\n",
    "    f = open(\"completed_tasks.txt\", \"a\")\n",
    "    f.write(pdb_id + '\\n')\n",
    "    f.close()\n",
    "\n",
    "# pka_calculate('1a79')\n",
    "# results_df = pd.concat([results_df, pka_to_pandas('1a79')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<span style=\"Times New Roman; font-size:2em;\">**Extracting results**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def pka_to_csv(pdb_id):\n",
    "    # Check whether the directory exists\n",
    "    file_name = result_directory + pdb_id + '.pka'\n",
    "    path_exists = Path(file_name).is_file()\n",
    "\n",
    "    if(not path_exists) :\n",
    "        print(\"File path not found:\" , file_name)\n",
    "    else:\n",
    "        # Reading the file if it exists\n",
    "        # Combine the file content as one string\n",
    "        file_text = ''\n",
    "        for line in open(file_name, 'r'):\n",
    "            file_text += line\n",
    "\n",
    "        # Splitting the result file by section\n",
    "        section = file_text.split('--------------------------------------------------------------------------------------------------------')\n",
    "        result_splitlines = section[1].splitlines()\n",
    "\n",
    "        # An array to store the result table\n",
    "        results = []\n",
    "        for string in result_splitlines[3:]:\n",
    "            list = [string[0:6].lstrip(),string[6:10].lstrip(),string[10:12].lstrip(),string[12:21].lstrip(),string[21:32].lstrip(), str(D(string[12:21].lstrip())-D(string[21:32].lstrip())), string[32:55].lstrip()]\n",
    "#             print(list)\n",
    "            results.append(list)\n",
    "\n",
    "        # Loading the array to pd dataframe\n",
    "        single_result_df = pd.DataFrame(results, columns = ['RESIDUE TYPE', 'RESIDUE NUMBER', 'CHAIN ID', 'pKa', 'model-pKa', 'delta-pKa', 'ligand atom-type'])\n",
    "\n",
    "        # Adding a column indicating the PDB ID\n",
    "        single_result_df.insert(0, 'PDB ID', pdb_id)\n",
    "\n",
    "        # Write csv file\n",
    "        single_result_df.to_csv( csv_directory + pdb_id + '.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Extracting results to csv\n",
    "for pdb_id in list_of_files:\n",
    "    # Logging events\n",
    "    print('Running:', pdb_id)\n",
    "    pka_to_csv(pdb_id)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<span style=\"Times New Roman; font-size:2em;\">**Concatenating Results**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "['12as.csv', '13pk.csv', '1a05.csv', '1a0i.csv', '1a16.csv', '1a26.csv', '1a30.csv', '1a41.csv', '1a4i.csv']\n"
     ]
    }
   ],
   "source": [
    "# checking the generated files in the csv_directory\n",
    "stream = os.popen('ls '+ csv_directory)\n",
    "output = stream.read()\n",
    "stream.close()\n",
    "list_of_csv = output.splitlines()\n",
    "print(len(list_of_csv))\n",
    "print(list_of_csv[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enzymes_without_cofactors = list_of_csv\n",
    "\n",
    "def remove_enzymes_without_cofactors(row):\n",
    "#     print(row)\n",
    "#     return row['residue/reactant/product/cofactor']\n",
    "    if row['residue/reactant/product/cofactor'] == 'cofactor':\n",
    "        pdb_file_name = row['PDB ID'] + '.csv'\n",
    "        if pdb_file_name in enzymes_without_cofactors:\n",
    "            enzymes_without_cofactors.remove(pdb_file_name)\n",
    "\n",
    "curated_df.apply(remove_enzymes_without_cofactors, axis=1)\n",
    "\n",
    "list_of_csv = enzymes_without_cofactors\n",
    "\n",
    "len(list_of_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "f = open(\"without_cofactors.txt\", \"a\")\n",
    "\n",
    "# Concatanate all generated csvs\n",
    "for csv_file in list_of_csv:\n",
    "    print('Running:', csv_file)\n",
    "    f.write(csv_file.replace('.csv','\\n'))\n",
    "\n",
    "f.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "f = open(\"results.csv\", \"a\")\n",
    "# f = open(\"enzymes_without_cofactors.csv\", \"a\")\n",
    "\n",
    "# Obtain the header from 1st file and write it\n",
    "first_file = open(csv_directory + list_of_csv[0] , \"r\").readlines()\n",
    "f.write(first_file[0])\n",
    "\n",
    "# Concatanate all generated csvs\n",
    "for csv_file in list_of_csv:\n",
    "    print('Running:', csv_file)\n",
    "    file = open(csv_directory + csv_file , \"r\").readlines()\n",
    "    del file[0]\n",
    "    for line in file:\n",
    "        f.write(line)\n",
    "#     f.write(csv_file.replace('.csv','\\n'))\n",
    "\n",
    "f.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<span style=\"Times New Roman; font-size:2em;\">**Checking Results**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Dataframe for storing the results\n",
    "results_df = pd.read_csv('results.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Dataframe for storing the results\n",
    "results_df = pd.read_csv('enzymes_without_cofactors.csv')\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}